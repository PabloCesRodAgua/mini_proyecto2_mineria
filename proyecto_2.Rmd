---
title: "Proyecto Minería de Datos"
author: "Santiago Delgado Ramos, Luis Armando Espinosa Mejía & Pablo César Rodríguez Aguayo"
date: "March 15, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<!-- Luis Espinosa --> 
## 1.- Basic Data Analysis

### 1.1.- Introduction 

### 1.2.- How data was recolected

####Limitations

####Disadvantages

### 1.3.- Basic Summary Statistics

### 1.4.- Boxplots Interpretation

### 1.5.- Skew of Data Interpretation

<!-- Santiago Delgado --> 
### 1.6.- Histograms Interpretation

### 1.7.- Quartiles Interpretation

### 1.8.- Correlation Interpretation

### 1.9.- Scatterplots Interpretation

<!-- Pablo Rodríguez --> 
## 2.- K - Nearest Neighbors
KNN is a supervised classified method that classifies by characteristics of unlabeled examples by assigning t
a class of similar label. The succes of this algorithm depends on the prediction, and the amount of data given.
But if the given data is noisy and there is no clear distinction among the groups, the nearest neighborg algorithms 
may struggle to identify the class boundaries.

Strenghts
- Simple and effective.
- Makes no assumptions abouthe the underlying data distribution.
- Fast training phase.

Weaknesses
- Doesn't produces a model, limiting the ability to understand how features are related to a class.
- Requieres selection of appropiate **k value**.
- Slow classification phase.
- Nominal features and missing data requiere addtional processing.


### 2.1.- Normalization
Before applying kNN algorithm, the first step is to apply a normalization method so features independent of their types get standarized. Lets say, if certain features have a larger range value than others, the distance measurements will be strongly dominated by the larger range values.  

For reasons of the Data Mining class, we will be using "Min Max Normalization" and "Z Score Normalization".

#### 2.1.1 Method 1 (Min Max Normaliztion)
The "min-max normalization" is a traditional method for process feature transformation such that all 
of its values fall in a range between 0 and 1. The formula for normalizing a feature is as follows:
$$x_{new} = \frac{x - min(x)}{max(x) - min(x)}$$
Basically it subtracts the minimum of feature x from each value and divides by the range of x.

```{r}
minmax_normalization <- function(x_value){
  return ((x - min(x_value)) / min(x_value) - max(x_value))
} # minmax_normalization

#students_questionary <- as.data.frame(lapply(data[,c(1,4)], minmax_normalizations))
```

#### 2.1.2 Method 2 (Z Score Normalization)
There is also "z-score standardization". Which subtracts the mean value of feature x, and divides the outcome by the standard deviation of x as follows:

$$x_{new} = \frac{x - \mu}{\sigma} - \frac{x - mean(x)}{stdev(x)}$$

Basically this method rescales each of the feature's values in terms of how many standard deviations they fall above or below the mean value. The resulting value is called a z-score. The z-scores fall in an unbound range of negative and positive numbers. Unlike the normalized values, they have no predefined minimum and maximum.

### 2.2.- Selected Features Explanation.

```{r}
```
### 2.3.- Compute Distances
Ase we saw in class there are many ways to compute the distance between two points, but for these case the k-NN
algorithm uses **Euclidean distance**, which is the distance one would measure if
it were possible to use a ruler to connect two points, illustrated in the previous
figure by the dotted lines connecting the wanted point to its neighbors.

```{r}
euclidean_distances <- dist(x = database, method = "euclidean")
print(euclidean_distances)
```

### 2.4.- Trainning & Testing Sets
```{r}

```

### 2.5.- Determination of the "Optimal K"
```{r}

```

### 2.6.- KNN Classification Outputs
```{r}

```

### 2.7 Frequency Table & Interpretation
```{r}

```
<!-- Santiago Delgado --> 
## 3.- Conclusions & Limitations
```{r}

```
