---
title: "proyecto_2_minería_de_datos"
author: "Santiago Delgado Ramos, Luis Armando Espinosa Mejía, Pablo César Rodríguez Aguayo"
date: "March 15, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## 1.- Basic Data Analysis

### 1.1.- Introduction 

### 1.2.- How data was recolected

# Limitations
# Disadvantages

### 1.3.- Basic Summary Statistics

### 1.4.- Boxplots Interpretation

### 1.5.- Skew of Data Interpretation

### 1.6.- Histograms Interpretation

### 1.7.- Quartiles Interpretation

### 1.8.- Correlation Interpretation

### 1.9.- Scatterplots Interpretation

## 2.- K - Nearest Neighbors
KNN is a supervised classified method that classifies by characteristics of unlabeled examples by assigning t
a class of similar label. The succes of this algorithm depends on the prediction, and the amount of data given.
But if the given data is noisy and there is no clear distinction among the groups, the nearest neighborg algorithms 
may struggle to identify the class boundaries.

Strenghts
- Simple and effective.
- Makes no assumptions abouthe the underlying data distribution.
- Fast training phase.

Weaknesses
- Doesn't produces a model, limiting the ability to understand how features are related to a class.
- Requieres selection of appropiate **k value**.
- Slow classification phase.
- Nominal features and missing data requiere addtional processing.



### 2.1.- Normalization
Before applying kNN algorithm, the first step is to apply a normalization method so features independent of their types get standarized. Lets say, if certain features have a larger range value than others, the distance measurements will be strongly dominated by the larger range values.  

For reasons of the Data Mining class, we will be using "Min Max Normalization" and "Z Score Normalization".

#### 2.1.1 Method 1 (Min Max Normaliztion)
The "min-max normalization" is a traditional method for process feature transformation such that all 
of its values fall in a range between 0 and 1. The formula for normalizing a feature is as follows:
$$x_{new} = \frac{x - min(x)}{max(x) - min(x)}$$
Basically it subtracts the minimum of feature x from each value and divides by the range of x.

```{r}
minmax_normalization <- function(x_value){
  return ((x - min(x_value)) / min(x_value) - max(x_value))
} # minmax_normalization
```

#### 2.1.2 Method 2 (Z Score Normalization)
There is also "z-score standardization". Which subtracts the mean value of feature x, and divides the outcome by the standard deviation of x as follows:
$$x_{new} = \frac{x - \mu}{\sigma} - \frac{x - mean(x)}{stdev(x)}$$
Basically this method rescales each of the feature's values in terms of how many standard deviations they fall above or below the mean value. The resulting value is called a z-score. The z-scores fall in an unbound range of negative and positive numbers. Unlike the normalized values, they have no predefined minimum and maximum.

### 2.2.- Selected Features Explanation.
```{r}
```
### 2.3.- Computer Distances
```{r}

```

### 2.4.- Trainning & Testing Sets
```{r}

```

### 2.5.- Determination of the "Optimal K"
```{r}

```

### 2.6.- KNN Classification Outputs
```{r}

```

### 2.7 Frequency Table & Interpretation
```{r}

```

## 3.- Conclusions & Limitations
```{r}

```
